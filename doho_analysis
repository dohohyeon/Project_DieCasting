import requests
import numpy as np
import pandas as pd
import geopandas as gpd
import time
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from rapidfuzz import fuzz
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from folium import Map, CircleMarker, LayerControl, FeatureGroup
import folium
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error
import pandas as pd, re, unicodedata
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.impute import KNNImputer
import scipy.stats as stats
from sklearn.ensemble import RandomForestClassifier
import scikit_posthocs as sp
from sklearn.metrics import precision_recall_curve, f1_score
from sklearn.preprocessing import RobustScaler
import optuna
from sklearn.model_selection import StratifiedKFold, cross_val_predict
from sklearn.metrics import precision_recall_curve, f1_score, classification_report, confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.inspection import PartialDependenceDisplay
import shap
import joblib

df = pd.read_excel("data/train.xlsx")
df = df.drop(index=19327) # ê²°ì¸¡ ë§ì€ ê°’ 1ê°œ
df[df['physical_strength'] == 65535] # 3ê°œ
df = df.drop(index=[6000,11811,17598])
df[df['low_section_speed'] == 65535] # 1ê°œ
df = df.drop(index=[46546])
list(df[df['Coolant_temperature'] == 1449].index) # 9ê°œ
df = df.drop(index=list(df[df['Coolant_temperature'] == 1449].index))
list(df[df['upper_mold_temp1'] == 1449].index) # 1ê°œ
df = df.drop(index=list(df[df['upper_mold_temp1'] == 1449].index))
list(df[df['upper_mold_temp2'] == 4232].index) # 1ê°œ
df = df.drop(index=list(df[df['upper_mold_temp2'] == 4232].index))

df['registration_time'] = pd.to_datetime(df['registration_time'])
df['hour'] = df['registration_time'].dt.hour
df['hour']=df['hour'].astype(object)


df.columns
df['tryshot_signal'] = df['tryshot_signal'].fillna('A')
df['molten_volume'] = df['molten_volume'].fillna(0)
condition = (df['molten_volume'].notna()) & (df['heating_furnace'].isna())
df.loc[condition, 'heating_furnace'] = 'C'

df["mold_code"] = df["mold_code"].astype(object)
df["EMS_operation_time"] = df["EMS_operation_time"].astype(object)
df.loc[df["molten_temp"] <= 80, "molten_temp"] = np.nan
df.loc[df["physical_strength"] <= 5, "physical_strength"] = np.nan

df = df.drop(columns=['id','line','name','mold_name','emergency_stop','time','date','registration_time',
                      'upper_mold_temp3','lower_mold_temp3','working'])

df.columns
df['low_section_speed'].hist()
np.sum(df['biscuit_thickness'] > 100)

# 2. ì…ë ¥ë³€ìˆ˜(X), íƒ€ê²Ÿë³€ìˆ˜(y) ë¶„ë¦¬
X = df.drop(columns=["passorfail"])
y = df["passorfail"]

split_point = int(len(df) * 0.8)
X_train = X.iloc[:split_point]
X_test = X.iloc[split_point:]
y_train = y.iloc[:split_point]
y_test = y.iloc[split_point:]

# 4. ìˆ˜ì¹˜í˜• / ë²”ì£¼í˜• ì»¬ëŸ¼ êµ¬ë¶„
num_cols = X_train.select_dtypes(include=["int64", "float64"]).columns
cat_cols = X_train.select_dtypes(include=["object"]).columns

# 5. ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì •ì˜
num_transformer = Pipeline(steps=[
    ("imputer", KNNImputer(n_neighbors=5)),  # âœ”ï¸ ìˆ˜ì¹˜í˜• â†’ KNN ê¸°ë°˜ ëŒ€ì¹˜
    ("scaler", RobustScaler())             # í‘œì¤€í™” ìŠ¤ì¼€ì¼ë§
])

cat_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")), # ë²”ì£¼í˜• â†’ ìµœë¹ˆê°’ ëŒ€ì¹˜
    ("onehot", OneHotEncoder(handle_unknown="ignore"))    # Trainì—ì„œ ë³¸ ì¹´í…Œê³ ë¦¬ë§Œ í•™ìŠµ
])

preprocessor = ColumnTransformer(
    transformers=[
        ("num", num_transformer, num_cols),
        ("cat", cat_transformer, cat_cols)
    ]
)

def objective(trial):
    # íƒìƒ‰í•  íŒŒë¼ë¯¸í„° ê³µê°„
    n_estimators = trial.suggest_int("n_estimators", 100, 500, step=50)
    max_depth = trial.suggest_int("max_depth", 3, 15)
    min_samples_split = trial.suggest_int("min_samples_split", 2, 10)
    min_samples_leaf = trial.suggest_int("min_samples_leaf", 1, 5)
    max_features = trial.suggest_categorical("max_features", ["sqrt", "log2", None])

    # ëª¨ë¸ ì •ì˜
    rf = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        min_samples_split=min_samples_split,
        min_samples_leaf=min_samples_leaf,
        max_features=max_features,
        class_weight="balanced",
        random_state=42,
        n_jobs=-1
    )

    # íŒŒì´í”„ë¼ì¸ (ì „ì²˜ë¦¬ + ëª¨ë¸)
    clf = Pipeline(steps=[
        ("preprocessor", preprocessor),
        ("classifier", rf)
    ])

    # êµì°¨ ê²€ì¦ F1 ì ìˆ˜ ê³„ì‚°
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    preds = cross_val_predict(clf, X_train, y_train, cv=cv, method="predict_proba")[:, 1]

    # threshold = 0.5 ê¸°ì¤€ì—ì„œ F1-score ê³„ì‚°
    preds_bin = (preds >= 0.5).astype(int)
    score = f1_score(y_train, preds_bin)

    return score

# -----------------------------
# 2) Optuna ì‹¤í–‰
# -----------------------------
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=30)

print("Best params:", study.best_params)
print("Best CV F1:", study.best_value)

# -----------------------------
# 3) ìµœì  ëª¨ë¸ í•™ìŠµ
# -----------------------------
best_params = study.best_params
rf_best = RandomForestClassifier(
    **best_params,
    class_weight="balanced",
    random_state=42,
    n_jobs=-1
)

clf_best = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("classifier", rf_best)
])

clf_best.fit(X_train, y_train)

# -----------------------------
# 4) ìµœì  threshold íƒìƒ‰
# -----------------------------
# ì˜ˆì¸¡ í™•ë¥ 
y_proba = clf_best.predict_proba(X_test)[:, 1]

# Precision-Recall ê³¡ì„  ê³„ì‚°
precisions, recalls, thresholds = precision_recall_curve(y_test, y_proba)

# ì¡°ê±´: Precision â‰¥ 0.8
precision_threshold = 0.8

valid_idx = np.where(precisions >= precision_threshold)[0]

if len(valid_idx) > 0:
    # ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” threshold ì¤‘ Recallì´ ìµœëŒ€ì¸ ì§€ì  ì„ íƒ
    best_idx = valid_idx[np.argmax(recalls[valid_idx])]
    best_threshold = thresholds[best_idx]
    best_precision = precisions[best_idx]
    best_recall = recalls[best_idx]
    print(f"ì¡°ê±´ ì¶©ì¡± threshold = {best_threshold:.3f}, Precision = {best_precision:.3f}, Recall = {best_recall:.3f}")
else:
    print("âš ï¸ Precision ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” thresholdê°€ ì—†ìŒ")


# ê°™ì€ íŒŒë¼ë¯¸í„° thresholdë¡œ smote ë°©ì‹ ì ìš©

# ì„ íƒëœ thresholdë¡œ ìµœì¢… ì˜ˆì¸¡ ìƒì„±
y_pred_opt = (y_proba >= best_threshold).astype(int)

# Confusion Matrix
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_opt))

# Classification Report
print("\nClassification Report:\n", classification_report(y_test, y_pred_opt))



# SMOTE ë°©ì‹
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
df = pd.read_excel("data/train.xlsx")
df = df.drop(index=19327) # ê²°ì¸¡ ë§ì€ ê°’ 1ê°œ
df[df['physical_strength'] == 65535] # 3ê°œ
df = df.drop(index=[6000,11811,17598])
df[df['low_section_speed'] == 65535] # 1ê°œ
df = df.drop(index=[46546])
list(df[df['Coolant_temperature'] == 1449].index) # 9ê°œ
df = df.drop(index=list(df[df['Coolant_temperature'] == 1449].index))
list(df[df['upper_mold_temp1'] == 1449].index) # 1ê°œ
df = df.drop(index=list(df[df['upper_mold_temp1'] == 1449].index))
list(df[df['upper_mold_temp2'] == 4232].index) # 1ê°œ
df = df.drop(index=list(df[df['upper_mold_temp2'] == 4232].index))

df['registration_time'] = pd.to_datetime(df['registration_time'])
df['hour'] = df['registration_time'].dt.hour
df['hour']=df['hour'].astype(object)


df.columns
df['tryshot_signal'] = df['tryshot_signal'].fillna('A')
df['molten_volume'] = df['molten_volume'].fillna(0)
condition = (df['molten_volume'].notna()) & (df['heating_furnace'].isna())
df.loc[condition, 'heating_furnace'] = 'C'

df["mold_code"] = df["mold_code"].astype(object)
df["EMS_operation_time"] = df["EMS_operation_time"].astype(object)
df.loc[df["molten_temp"] <= 80, "molten_temp"] = np.nan
df.loc[df["physical_strength"] <= 5, "physical_strength"] = np.nan

df = df.drop(columns=['id','line','name','mold_name','emergency_stop','time','date','registration_time',
                      'upper_mold_temp3','lower_mold_temp3','working'])
# -----------------------------
# 1) ì…ë ¥ë³€ìˆ˜(X), íƒ€ê²Ÿë³€ìˆ˜(y) ë¶„ë¦¬
# -----------------------------
X = df.drop(columns=["passorfail"])
y = df["passorfail"]

split_point = int(len(df) * 0.8)
X_train = X.iloc[:split_point]
X_test = X.iloc[split_point:]
y_train = y.iloc[:split_point]
y_test = y.iloc[split_point:]

# -----------------------------
# 2) ìˆ˜ì¹˜í˜• / ë²”ì£¼í˜• ì»¬ëŸ¼ êµ¬ë¶„
# -----------------------------
num_cols = X_train.select_dtypes(include=["int64", "float64"]).columns
cat_cols = X_train.select_dtypes(include=["object"]).columns

# ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
num_transformer = Pipeline(steps=[
    ("imputer", KNNImputer(n_neighbors=5)),
    ("scaler", RobustScaler())
])

cat_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

preprocessor = ColumnTransformer(
    transformers=[
        ("num", num_transformer, num_cols),
        ("cat", cat_transformer, cat_cols)
    ]
)

# -----------------------------
# 3) Optuna ëª©ì  í•¨ìˆ˜ ì •ì˜
# -----------------------------
def objective(trial):
    n_estimators = trial.suggest_int("n_estimators", 100, 500, step=50)
    max_depth = trial.suggest_int("max_depth", 3, 15)
    min_samples_split = trial.suggest_int("min_samples_split", 2, 10)
    min_samples_leaf = trial.suggest_int("min_samples_leaf", 1, 5)
    max_features = trial.suggest_categorical("max_features", ["sqrt", "log2", None])

    rf = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        min_samples_split=min_samples_split,
        min_samples_leaf=min_samples_leaf,
        max_features=max_features,
        random_state=42,
        n_jobs=-1
    )

    clf = ImbPipeline(steps=[
        ("preprocessor", preprocessor),
        ("smote", SMOTE(random_state=42)),
        ("classifier", rf)
    ])

    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    preds = cross_val_predict(clf, X_train, y_train, cv=cv, method="predict_proba")[:, 1]

    preds_bin = (preds >= 0.5).astype(int)
    score = f1_score(y_train, preds_bin)

    return score

# -----------------------------
# 4) Optuna ì‹¤í–‰
# -----------------------------
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=30)

print("Best params:", study.best_params)
print("Best CV F1:", study.best_value)

# -----------------------------
# 5) ìµœì  ëª¨ë¸ í•™ìŠµ (SMOTE í¬í•¨)
# -----------------------------
best_params = study.best_params
rf_best = RandomForestClassifier(
    **best_params,
    random_state=42,
    n_jobs=-1
)

clf_best = ImbPipeline(steps=[
    ("preprocessor", preprocessor),
    ("smote", SMOTE(random_state=42)),
    ("classifier", rf_best)
])

clf_best.fit(X_train, y_train)

# -----------------------------
# 6) ìµœì  threshold íƒìƒ‰ (Precision ì¡°ê±´)
# -----------------------------
y_proba = clf_best.predict_proba(X_test)[:, 1]

precisions, recalls, thresholds = precision_recall_curve(y_test, y_proba)

# Precision í•˜í•œì„ 
precision_threshold = 0.8
valid_idx = np.where(precisions >= precision_threshold)[0]

if len(valid_idx) > 0:
    best_idx = valid_idx[np.argmax(recalls[valid_idx])]
    best_threshold = thresholds[best_idx]
    best_precision = precisions[best_idx]
    best_recall = recalls[best_idx]
    print(f"ì¡°ê±´ ì¶©ì¡± threshold = {best_threshold:.3f}, Precision = {best_precision:.3f}, Recall = {best_recall:.3f}")
    
    # ìµœì¢… ì˜ˆì¸¡
    y_pred_opt = (y_proba >= best_threshold).astype(int)

    print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_opt))
    print("\nClassification Report:\n", classification_report(y_test, y_pred_opt))
else:
    print("âš ï¸ Precision ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” thresholdê°€ ì—†ìŒ")


# Logistic Regression
df = pd.read_excel("data/train.xlsx")
# ì´ìƒì¹˜/ê²°ì¸¡ í–‰ ì œê±°
df = df.drop(index=19327) 
df = df.drop(index=[6000,11811,17598]) 
df = df.drop(index=[46546])
df = df.drop(index=list(df[df['Coolant_temperature'] == 1449].index))
df = df.drop(index=list(df[df['upper_mold_temp1'] == 1449].index))
df = df.drop(index=list(df[df['upper_mold_temp2'] == 4232].index))

# ì‹œê°„ ë³€ìˆ˜ ê°€ê³µ
df['registration_time'] = pd.to_datetime(df['registration_time'])
df['hour'] = df['registration_time'].dt.hour.astype(object)

# ê²°ì¸¡ì¹˜ ë³´ì •
df['tryshot_signal'] = df['tryshot_signal'].fillna('A')
df['molten_volume'] = df['molten_volume'].fillna(0)
condition = (df['molten_volume'].notna()) & (df['heating_furnace'].isna())
df.loc[condition, 'heating_furnace'] = 'C'

# íƒ€ì… ë³€ê²½
df["mold_code"] = df["mold_code"].astype(object)
df["EMS_operation_time"] = df["EMS_operation_time"].astype(object)

# ê°’ ì¡°ê±´ ê¸°ë°˜ ê²°ì¸¡ ì²˜ë¦¬
df.loc[df["molten_temp"] <= 80, "molten_temp"] = np.nan
df.loc[df["physical_strength"] <= 5, "physical_strength"] = np.nan

# ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°
df = df.drop(columns=[
    'id','line','name','mold_name','emergency_stop','time','date','registration_time',
    'upper_mold_temp3','lower_mold_temp3','working'
])
df.info()
# -------------------
# 1) X, y ë¶„ë¦¬
# -------------------
y = df['passorfail']
X = df.drop(columns=['passorfail'])

# ì‹œê°„ ìˆœì„œ ê¸°ì¤€ìœ¼ë¡œ split 
split_point = int(len(df) * 0.8)
X_train, X_test = X.iloc[:split_point], X.iloc[split_point:]
y_train, y_test = y.iloc[:split_point], y.iloc[split_point:]

# 4. ìˆ˜ì¹˜í˜• / ë²”ì£¼í˜• ì»¬ëŸ¼ êµ¬ë¶„
num_cols = X_train.select_dtypes(include=["int64", "float64"]).columns
cat_cols = X_train.select_dtypes(include=["object"]).columns

# 5. ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì •ì˜
num_transformer = Pipeline(steps=[
    ("imputer", KNNImputer(n_neighbors=5)),  # âœ”ï¸ ìˆ˜ì¹˜í˜• â†’ KNN ê¸°ë°˜ ëŒ€ì¹˜
    ("scaler", RobustScaler())             # í‘œì¤€í™” ìŠ¤ì¼€ì¼ë§
])

cat_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")), # ë²”ì£¼í˜• â†’ ìµœë¹ˆê°’ ëŒ€ì¹˜
    ("onehot", OneHotEncoder(handle_unknown="ignore"))    # Trainì—ì„œ ë³¸ ì¹´í…Œê³ ë¦¬ë§Œ í•™ìŠµ
])

preprocessor = ColumnTransformer(
    transformers=[
        ("num", num_transformer, num_cols),
        ("cat", cat_transformer, cat_cols)
    ]
)

from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score

# 6. ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ ì •ì˜
log_reg = LogisticRegression(
    max_iter=1000,          # ë°˜ë³µ íšŸìˆ˜ (ìˆ˜ë ´ ë³´ì¥)
    class_weight="balanced", # ë¶ˆê· í˜• ë°ì´í„° ë³´ì •
    solver="lbfgs",         # ë‹¤í•­/ì›í•« ì¸ì½”ë”© ì§€ì›
    random_state=42
)

# 7. ì „ì²´ íŒŒì´í”„ë¼ì¸ êµ¬ì„± (ì „ì²˜ë¦¬ + ë¡œì§€ìŠ¤í‹± íšŒê·€)
clf = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("classifier", log_reg)
])

# 8. ëª¨ë¸ í•™ìŠµ
clf.fit(X_train, y_train)

# 9. ì˜ˆì¸¡
y_pred = clf.predict(X_test)
y_proba = clf.predict_proba(X_test)[:, 1]

# 10. í‰ê°€ ì§€í‘œ ì¶œë ¥
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nROC AUC:", roc_auc_score(y_test, y_proba))

# 1. ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
log_reg_model = clf.named_steps["classifier"]

# 2. ì „ì²´ feature ì´ë¦„ ì¶”ì¶œ
feature_names = clf.named_steps["preprocessor"].get_feature_names_out()

# 3. ìˆ˜ì¹˜í˜• feature ì´ë¦„ë§Œ ì¶”ì¶œ
num_feature_names = clf.named_steps["preprocessor"].transformers_[0][1] \
                        .get_feature_names_out(num_cols)

# 4. coef_ì™€ ë§¤ì¹­
coefficients = log_reg_model.coef_[0]

coef_df = pd.DataFrame({
    "feature": feature_names,
    "coefficient": coefficients
})

# 5. ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë§Œ í•„í„°ë§
coef_num = coef_df[coef_df["feature"].str.startswith("num__")] \
              .sort_values(by="coefficient", ascending=False)

print(coef_num)
coef_num['coefficient_abs']=np.abs(coef_num['coefficient'])
coef_num.sort_values('coefficient_abs',ascending=False)


# ----------------------------------
# (A) PDP Plot (ëª¨ë“  ë³€ìˆ˜ì— ëŒ€í•´)
# ----------------------------------

import pandas as pd
import joblib
import matplotlib.pyplot as plt
from sklearn.inspection import PartialDependenceDisplay
import warnings
warnings.filterwarnings('ignore')

# -----------------------------
# 1. ë°ì´í„° & ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
# -----------------------------0
# df = pd.read_csv("data/train.csv", encoding="utf-8", low_memory=False)
df = pd.read_excel("data/train.xlsx")
clf = joblib.load("model.pkl")   # íŒŒì´í”„ë¼ì¸ ì „ì²´ (ì „ì²˜ë¦¬+ëª¨ë¸)

# ì‚¬ìš© ë³€ìˆ˜ (ì›ë³¸ ë°ì´í„° ì»¬ëŸ¼)
feature_columns = [
    "cast_pressure",
    "count", 
    "upper_mold_temp1",
    "low_section_speed",
    "lower_mold_temp2",
    "high_section_speed",
    "upper_mold_temp2",
    "lower_mold_temp1",
    "biscuit_thickness",
    "sleeve_temperature"
]

X_original = df[feature_columns].dropna()

# -----------------------------
# 2. ì „ì²˜ë¦¬ + ëª¨ë¸ ë¶„ë¦¬
# -----------------------------
preprocessor = clf.named_steps["preprocessor"]
model = clf.named_steps["classifier"]

# ì „ì²˜ë¦¬ ì ìš© (PDPëŠ” classifier input ê³µê°„ì—ì„œ ê³„ì‚°)
X_transformed = preprocessor.transform(X_original)

# ë³€í™˜ëœ feature ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
try:
    transformed_feature_names = preprocessor.get_feature_names_out()
except:
    transformed_feature_names = [f"feature_{i}" for i in range(X_transformed.shape[1])]

# -----------------------------
# 3. PDP ê·¸ë¦¬ê¸°
# -----------------------------

plt.rc("font", family="Malgun Gothic")   # ë§‘ì€ ê³ ë”•
plt.rc("axes", unicode_minus=False)  # ë§ˆì´ë„ˆìŠ¤(-) ê¸°í˜¸ ê¹¨ì§ ë°©ì§€

import matplotlib.pyplot as plt
from sklearn.inspection import PartialDependenceDisplay

loaded_model = joblib.load("model.pkl")
loaded_model['feature_names']

# 2) PDPë¥¼ ê·¸ë¦¬ê³  ì‹¶ì€ ë³€ìˆ˜ë“¤ ì§€ì • (ì˜ˆì‹œ: 2ê°œ ë³€ìˆ˜)
best_features = loaded_model['feature_names']

features_to_plot = loaded_model['feature_names']
# 3) PDP Plot
fig, ax = plt.subplots(figsize=(20, 12))
PartialDependenceDisplay.from_estimator(
    estimator=loaded_model,             # ë¶ˆëŸ¬ì˜¨ ëª¨ë¸
    X=X_train[best_features],           # í•™ìŠµì— ì‚¬ìš©í•œ ë…ë¦½ë³€ìˆ˜ ë°ì´í„°
    features=features_to_plot,          # PDP ë³´ê³  ì‹¶ì€ ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸
    percentiles=(0.05, 0.95),           # ê·¹ë‹¨ê°’ ì œì™¸
    ax=ax
)

X_train.info()
plt.suptitle("Partial Dependence Plot", fontsize=16)
plt.tight_layout()


# ëª¨ë¸ê³¼ feature ë¦¬ìŠ¤íŠ¸ ë¶„ë¦¬
loaded = joblib.load("model.pkl")
loaded_model = loaded["model"]              # ì‹¤ì œ ëª¨ë¸(Pipeline or Estimator)
best_features = loaded["feature_names"]     # feature ì´ë¦„ ë¦¬ìŠ¤íŠ¸

# PDP ê·¸ë¦¬ê¸°
fig, ax = plt.subplots(figsize=(20, 12))
PartialDependenceDisplay.from_estimator(
    estimator=loaded_model,             # âœ… ì‹¤ì œ ëª¨ë¸ ê°ì²´
    X=X_train,           # í•™ìŠµì— ì‚¬ìš©í•œ ë…ë¦½ë³€ìˆ˜
    features=["molten_volume", "physical_strength"],  # PDP ê·¸ë¦´ ëŒ€ìƒ
    percentiles=(0.05, 0.95),
    ax=ax
)

plt.suptitle("Partial Dependence Plot", fontsize=16)
plt.tight_layout()
plt.show()



# 1. ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
clf_loaded = joblib.load("model.pkl")
print(clf_loaded)

# 2. feature ì´ë¦„ í™•ë³´
if hasattr(clf_loaded, "named_steps") and "preprocessor" in clf_loaded.named_steps:
    preprocessor = clf_loaded.named_steps["preprocessor"]
    feature_names = preprocessor.get_feature_names_out()
else:
    feature_names = X_test.columns  # fallback (ëª¨ë¸ë§Œ ì €ì¥ëœ ê²½ìš°)

print("Features:", feature_names)


# ----------------------------------
# (B) SHAP Value Plot (ëª¨ë“  ë³€ìˆ˜ì— ëŒ€í•´)
# ----------------------------------
# 1) classifier ì¶”ì¶œ
if hasattr(clf_loaded, "named_steps") and "classifier" in clf_loaded.named_steps:
    model = clf_loaded.named_steps["classifier"]
else:
    model = clf_loaded

# 2) X_test ì „ì²˜ë¦¬ (Pipelineì´ë©´ transform í•„ìš”)
if hasattr(clf_loaded, "named_steps") and "preprocessor" in clf_loaded.named_steps:
    X_transformed = preprocessor.transform(X_test)
    X_transformed = pd.DataFrame(X_transformed, columns=feature_names)
else:
    X_transformed = X_test.copy()

# 3) ìƒ˜í”Œë§ (ì†ë„ ë¬¸ì œ ë°©ì§€)
X_sample = X_transformed.sample(200, random_state=42)

# 4) SHAP í•´ì„
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_sample)

# Summary plot (bar)
shap.summary_plot(shap_values, X_sample, plot_type="bar")

# Summary plot (dot scatter)
shap.summary_plot(shap_values, X_sample)


# Shap Force Plot

clf_loaded = joblib.load("model.pkl")
print(clf_loaded)

# 1. ëª¨ë¸ê³¼ ì „ì²˜ë¦¬ê¸° ê°€ì ¸ì˜¤ê¸°
preprocessor = clf_loaded.named_steps["preprocessor"]
model = clf_loaded.named_steps["classifier"]

# 2. ì‚¬ìš©ì ì •ì˜ ìƒ˜í”Œ (ì›ë³¸ feature ê¸°ì¤€)
custom_sample = pd.DataFrame([{
    "molten_temp": 720,
    "facility_operation_cycleTime": 35,
    "production_cycletime": 42,
    "tryshot_signal": "A",
    "mold_code": "8412",
    # ğŸ‘‰ ì‹¤ì œ X_testì— ìˆëŠ” ëª¨ë“  ë³€ìˆ˜ í¬í•¨í•´ì•¼ í•¨
}])

# 3. ì „ì²˜ë¦¬ ì ìš©
X_custom = preprocessor.transform(custom_sample)
X_custom = pd.DataFrame(X_custom, columns=preprocessor.get_feature_names_out())

# 4. SHAP ê°’ ê³„ì‚°
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_custom)

# 5. Force plot ê·¸ë¦¬ê¸°
shap.initjs()
shap.force_plot(
    explainer.expected_value,
    shap_values[0, :],       # ì²« ë²ˆì§¸(ìœ ì¼í•œ) ìƒ˜í”Œ
    X_custom.iloc[0, :],
    matplotlib=True
)



final_model = final_model = joblib.load("model.pkl")
feature_names = final_model['feature_names']

# ì‹¤ì œ ëª¨ë¸ ê°ì²´ ì¶”ì¶œ (ì¼ë°˜ì ìœ¼ë¡œ 'model' ë˜ëŠ” 'estimator' í‚¤ì— ì €ì¥ë¨)
model = final_model['model']  # ë˜ëŠ” final_model['estimator']

X_sample = X_train.sample(3000, random_state=42)  
PartialDependenceDisplay.from_estimator(
    model,  # ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ì‹¤ì œ ëª¨ë¸ ê°ì²´ ì‚¬ìš©
    X_sample,
    features=["upper_mold_temp2"],
    feature_names=feature_names,
    grid_resolution=20
)
plt.show()


def calculate_vif(X):
    vif_data = pd.DataFrame()
    vif_data["feature"] = X.columns
    vif_data["VIF"] = [
        variance_inflation_factor(X.values, i)
        for i in range(X.shape[1])
    ]
    return vif_data

X_num = X_train[num_cols].fillna(X_train[num_cols].mean())

# ë‹¨ê³„ì ìœ¼ë¡œ VIF ì¤„ì´ê¸°
threshold = 10
while True:
    vif = calculate_vif(X_num)
    max_vif = vif["VIF"].max()
    if max_vif > threshold:
        drop_feature = vif.sort_values("VIF", ascending=False).iloc[0]["feature"]
        print(f"ì œê±°: {drop_feature} (VIF={max_vif:.2f})")
        X_num = X_num.drop(columns=[drop_feature])
    else:
        break

print("ìµœì¢… ë³€ìˆ˜ ëª©ë¡:", X_num.columns.tolist())

from statsmodels.stats.outliers_influence import variance_inflation_factor

X_num = X_train[num_cols].copy()

# NaN ìˆìœ¼ë©´ ì„ì‹œë¡œ í‰ê·  ëŒ€ì¹˜ (VIFëŠ” NaN í—ˆìš©X)
X_num = X_num.fillna(X_num.mean())

# VIF ê³„ì‚°
vif_data = pd.DataFrame()
vif_data["feature"] = X_num.columns
vif_data["VIF"] = [
    variance_inflation_factor(X_num.values, i)
    for i in range(X_num.shape[1])
]

print(vif_data.sort_values("VIF", ascending=False))

plt.figure(figsize=(10,8))
sns.heatmap(X_train[num_cols].corr(), annot=True, cmap="coolwarm")
plt.show()



corr_matrix = X_train[num_cols].corr()

# 2. ì ˆëŒ€ê°’ 0.5 ì´ìƒì¸ ê°’ë§Œ ì¶”ì¶œ (ìê¸°ìì‹ =1.0ì€ ì œì™¸)
high_corr = corr_matrix[(np.abs(corr_matrix) >= 0.5) & (corr_matrix != 1.0)]

# 3. NaN ê°’ ì œê±° (í•„ìš”ì‹œ)
high_corr = high_corr.dropna(how="all").dropna(axis=1, how="all")

print(high_corr)


import optuna
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier

# 1) ëª©ì  í•¨ìˆ˜ ì •ì˜
def objective(trial):
    # íŠœë‹í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ê³µê°„ ì •ì˜
    n_estimators = trial.suggest_int("n_estimators", 100, 500, step=50)
    max_depth = trial.suggest_int("max_depth", 3, 15)
    min_samples_split = trial.suggest_int("min_samples_split", 2, 10)
    min_samples_leaf = trial.suggest_int("min_samples_leaf", 1, 5)
    max_features = trial.suggest_categorical("max_features", ["sqrt", "log2", None])

    # ëª¨ë¸ ì •ì˜
    rf = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        min_samples_split=min_samples_split,
        min_samples_leaf=min_samples_leaf,
        max_features=max_features,
        class_weight="balanced",
        random_state=42,
        n_jobs=-1
    )

    # ì „ì²´ íŒŒì´í”„ë¼ì¸ êµ¬ì„± (ì „ì²˜ë¦¬ + ëª¨ë¸)
    clf = Pipeline(steps=[
        ("preprocessor", preprocessor),
        ("classifier", rf)
    ])

    # êµì°¨ê²€ì¦ (Stratified KFold ì‚¬ìš©)
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    scores = cross_val_score(clf, X_train, y_train, cv=cv, scoring="f1")

    return scores.mean()

# 2) Optuna ì‹¤í–‰
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=30)

# 3) ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶œë ¥
print("Best params:", study.best_params)
print("Best F1-score:", study.best_value)

# 4) ìµœì  ëª¨ë¸ë¡œ í•™ìŠµ
best_params = study.best_params
rf_best = RandomForestClassifier(
    **best_params,
    class_weight="balanced",
    random_state=42,
    n_jobs=-1
)

clf_best = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("classifier", rf_best)
])

clf_best.fit(X_train, y_train)
y_pred = clf_best.predict(X_test)

print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))


df = pd.read_excel("data/train.xlsx")
# ì´ìƒì¹˜/ê²°ì¸¡ í–‰ ì œê±°
df = df.drop(index=19327) 
df = df.drop(index=[6000,11811,17598]) 
df = df.drop(index=[46546])
df = df.drop(index=list(df[df['Coolant_temperature'] == 1449].index))
df = df.drop(index=list(df[df['upper_mold_temp1'] == 1449].index))
df = df.drop(index=list(df[df['upper_mold_temp2'] == 4232].index))

# ì‹œê°„ ë³€ìˆ˜ ê°€ê³µ
df['registration_time'] = pd.to_datetime(df['registration_time'])
df['hour'] = df['registration_time'].dt.hour.astype(object)

# ê²°ì¸¡ì¹˜ ë³´ì •
df['tryshot_signal'] = df['tryshot_signal'].fillna('A')
df['molten_volume'] = df['molten_volume'].fillna(0)
condition = (df['molten_volume'].notna()) & (df['heating_furnace'].isna())
df.loc[condition, 'heating_furnace'] = 'C'

# íƒ€ì… ë³€ê²½
df["mold_code"] = df["mold_code"].astype(object)
df["EMS_operation_time"] = df["EMS_operation_time"].astype(object)

# ê°’ ì¡°ê±´ ê¸°ë°˜ ê²°ì¸¡ ì²˜ë¦¬
df.loc[df["molten_temp"] <= 80, "molten_temp"] = np.nan
df.loc[df["physical_strength"] <= 5, "physical_strength"] = np.nan

# ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°
df = df.drop(columns=[
    'id','line','name','mold_name','emergency_stop','time','date','registration_time',
    'upper_mold_temp3','lower_mold_temp3','working'
])
df.info()

(df['passorfail']==1).sum()/len(df) # ì „ì²´ ë¶ˆëŸ‰ë¥ 

mold_defect = (
    df.groupby("mold_code")["passorfail"]
    .mean()  # ë¶ˆëŸ‰ë¥ (= ë¶ˆëŸ‰ê±´ìˆ˜/ì „ì²´ê±´ìˆ˜)
    .reset_index()
    .rename(columns={"passorfail": "ë¶ˆëŸ‰ë¥ "})
)

# 2) ë¶ˆëŸ‰ë¥  ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
mold_defect = mold_defect.sort_values("ë¶ˆëŸ‰ë¥ ", ascending=False)

plt.figure(figsize=(10,6))
sns.barplot(x="mold_code", y="ë¶ˆëŸ‰ë¥ ", data=mold_defect)  # Seabornìœ¼ë¡œ ë§‰ëŒ€ê·¸ë˜í”„
plt.xticks(rotation=45)   # Matplotlibìœ¼ë¡œ xì¶• ê¸€ì íšŒì „
plt.ylabel("ë¶ˆëŸ‰ë¥ ")
plt.title("mold_codeë³„ ë¶ˆëŸ‰ë¥ ")
plt.ylim(0.03, 0.06)           # yì¶• ë²”ìœ„ 0 ~ 30ìœ¼ë¡œ ì œí•œ
plt.tight_layout()
plt.show()




# -----------------------------
# 1) ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
# -----------------------------
final_model = joblib.load("final_model.pkl")
feature_names = final_model['feature_names']
pipeline = final_model['model']

model = pipeline.named_steps['classifier']
preprocessor = pipeline.named_steps['preprocessor']

# -----------------------------
# 2) íŠ¹ì • mold_code í•„í„°ë§
# -----------------------------
TARGET_MOLD = 8722

df_seg = df[df["mold_code"] == TARGET_MOLD].copy()
X_seg = df_seg[feature_names]  # ì¼ë‹¨ ì „ì²´ feature ì‚¬ìš©
y_seg = df_seg["passorfail"]

# ë¶ˆëŸ‰ë¥  ì¶œë ¥
defect_rate = y_seg.mean() * 100
print(f"mold_code {TARGET_MOLD} ë¶ˆëŸ‰ë¥ : {defect_rate:.2f}%")
print(f"ì „ì²´ ìƒ˜í”Œ ìˆ˜: {len(df_seg)}ê°œ (ë¶ˆëŸ‰: {y_seg.sum()}ê°œ)")

# -----------------------------
# 3) SHAP ë¶„ì„
# -----------------------------
# ì „ì²˜ë¦¬
X_seg_transformed = preprocessor.transform(X_seg)

# feature names ê°€ì ¸ì˜¤ê¸°
try:
    transformed_feature_names = list(preprocessor.get_feature_names_out())
except:
    transformed_feature_names = 11

# SHAP ê³„ì‚°
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_seg_transformed)

# ì´ì§„ ë¶„ë¥˜: class 1(ë¶ˆëŸ‰) ì„ íƒ
shap_values_plot = shap_values[1] if isinstance(shap_values, list) else shap_values

# -----------------------------
# 4) TOP 10 ë³€ìˆ˜ë§Œ ì‹œê°í™” (mold_code ì œì™¸)
# -----------------------------
TOP_N = 10

# í‰ê·  ì ˆëŒ“ê°’ìœ¼ë¡œ ì¤‘ìš”ë„ ê³„ì‚°
shap_mean_abs = np.abs(shap_values_plot).mean(axis=0)

# mold_code ê´€ë ¨ ì»¬ëŸ¼ ì¸ë±ìŠ¤ ì°¾ê¸°
mold_code_indices = [i for i, name in enumerate(transformed_feature_names) 
                     if 'mold_code' in name.lower()]

# mold_code ì œì™¸í•œ ì¸ë±ìŠ¤
non_mold_indices = [i for i in range(len(transformed_feature_names)) 
                    if i not in mold_code_indices]

# mold_code ì œì™¸í•œ ë°ì´í„°
shap_values_filtered = shap_values_plot[:, non_mold_indices]
X_seg_filtered = X_seg_transformed[:, non_mold_indices]
feature_names_filtered = [transformed_feature_names[i] for i in non_mold_indices]
shap_mean_abs_filtered = shap_mean_abs[non_mold_indices]

# ìƒìœ„ Nê°œ ì¸ë±ìŠ¤ (í•„í„°ë§ëœ ë°ì´í„° ê¸°ì¤€)
top_indices = np.argsort(shap_mean_abs_filtered)[-TOP_N:][::-1]

# í•„í„°ë§
shap_values_top = shap_values_filtered[:, top_indices]
X_seg_top = X_seg_filtered[:, top_indices]
feature_names_top = [feature_names_filtered[i] for i in top_indices]

# ì‹œê°í™”
plt.figure(figsize=(10, 6))
shap.summary_plot(shap_values_top, X_seg_top, 
                  feature_names=feature_names_top, 
                  plot_type="bar", show=False)
plt.title(f"mold_code {TARGET_MOLD} - SHAP ë³€ìˆ˜ ì˜í–¥ë„ TOP {TOP_N}")
plt.tight_layout()
plt.show()



# ê²°ê³¼ ì¶œë ¥
print(f"\n=== TOP {TOP_N} ë¶ˆëŸ‰ ì˜í–¥ ë³€ìˆ˜ (mold_code ì œì™¸) ===")
for i, idx in enumerate(top_indices):
    print(f"{i+1}. {feature_names_filtered[idx]}: {shap_mean_abs_filtered[idx]:.4f}")


# ========== ì„¤ì • ==========
TARGET_MOLD = 8722
TARGET_FEATURE = 'sleeve_temperature'   # ë¶„ì„í•  ë³€ìˆ˜
SAMPLE_SIZE = 3000                      # ìƒ˜í”Œ ìˆ˜ (Noneì´ë©´ ì „ì²´ ì‚¬ìš©)
EPS = 0.001                             # ê¶Œì¥ êµ¬ê°„ í—ˆìš© ì˜¤ì°¨ (PDP ìµœì €ì¹˜ + Îµ)
GRID = 50                               # PDP ê²©ì ìˆ˜
# ==========================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import joblib
from sklearn.inspection import PartialDependenceDisplay, partial_dependence

# -----------------------------
# 1. ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
# -----------------------------
final_model = joblib.load("final_model.pkl")
feature_names = final_model['feature_names']
pipeline = final_model['model']   # Pipeline(ì „ì²˜ë¦¬+ëª¨ë¸)

# -----------------------------
# 2. íŠ¹ì • mold_code í•„í„°ë§
# -----------------------------
df_seg = df[df["mold_code"] == TARGET_MOLD].copy()

# ìƒ˜í”Œë§
if SAMPLE_SIZE is not None and len(df_seg) > SAMPLE_SIZE:
    total = len(df_seg)
    df_seg = df_seg.sample(n=SAMPLE_SIZE, random_state=42)
    print(f"ìƒ˜í”Œë§: {SAMPLE_SIZE}ê°œ ì‚¬ìš© (ì„¸ê·¸ë¨¼íŠ¸ ì „ì²´ {total}ê°œ)")

X_seg = df_seg[feature_names]
y_seg = df_seg["passorfail"]

print(f"mold_code {TARGET_MOLD} ìƒ˜í”Œ ìˆ˜: {len(df_seg)}ê°œ")
print(f"ë¶ˆëŸ‰ë¥ : {y_seg.mean() * 100:.2f}%\n")

# -----------------------------
# 3. PDP + Actionable Insight
# -----------------------------
if TARGET_FEATURE not in feature_names:
    print(f"[ì˜¤ë¥˜] '{TARGET_FEATURE}' ë³€ìˆ˜ê°€ feature_namesì— ì—†ìŠµë‹ˆë‹¤.")
    print("ì‚¬ìš© ê°€ëŠ¥í•œ ë³€ìˆ˜ ëª©ë¡ ì˜ˆì‹œ:", feature_names[:10], "...")
else:
    print(f"PDP ëŒ€ìƒ ë³€ìˆ˜: {TARGET_FEATURE}")

    # 3-1) PDP ê·¸ë˜í”„ (ê¸°ë³¸ ê³¡ì„ )
    display = PartialDependenceDisplay.from_estimator(
        estimator=pipeline,
        X=X_seg,
        features=[TARGET_FEATURE],
        kind='average',
        grid_resolution=GRID,
        response_method="predict_proba"
    )
    ax = display.axes_[0, 0]   # ì‹¤ì œ ê·¸ë ¤ì§„ ì¶•ì„ ê°€ì ¸ì˜´

    # 3-2) PDP ê³¡ì„  ê°’ ì§ì ‘ ì¶”ì¶œ
    pd_res = partial_dependence(
        estimator=pipeline,
        X=X_seg,
        features=[TARGET_FEATURE],
        kind='average',
        grid_resolution=GRID,
        response_method="predict_proba"
    )
    xx = np.asarray(pd_res["grid_values"][0])  # xì¶• grid ê°’
    yy = np.asarray(pd_res["average"][0])      # PDP í‰ê·  ê³¡ì„  ê°’

    # 3-3) ë°ì´í„° ë¶„í¬ ê¸°ë°˜ binning (í‘œë³¸ ìˆ˜ ê³ ë ¤)
    xvals = X_seg[TARGET_FEATURE].values
    edges = np.r_[xx[0], (xx[1:] + xx[:-1]) / 2, xx[-1]]   # bin ê²½ê³„
    bin_ids = np.digitize(xvals, edges) - 1
    bin_ids = np.clip(bin_ids, 0, len(xx)-1)
    counts = np.bincount(bin_ids, minlength=len(xx))

    # ìµœì†Œ í‘œë³¸ ìˆ˜ (10ê°œ ì´ìƒ, ë˜ëŠ” ì „ì²´ì˜ 1% ì´ìƒ)
    MIN_BIN = max(10, int(0.01 * len(X_seg)))
    dense_mask = counts >= MIN_BIN

    # 3-4) ê¶Œì¥ êµ¬ê°„ ê³„ì‚° (PDP ìµœì €ì¹˜+Îµ AND ì¶©ë¶„í•œ í‘œë³¸ìˆ˜)
    ymin = float(yy.min())
    mask_good = (yy <= (ymin + EPS)) & dense_mask

    good_lo, good_hi = None, None
    if mask_good.any():
        idx = np.where(mask_good)[0]
        splits = np.where(np.diff(idx) != 1)[0] + 1
        runs = np.split(idx, splits)
        best = max(runs, key=len)   # ê°€ì¥ ê¸´ ì—°ì† êµ¬ê°„ ì„ íƒ
        good_lo, good_hi = xx[best[0]], xx[best[-1]]

    # 3-5) ê·¸ë˜í”„ì— ê¶Œì¥ êµ¬ê°„ í‘œì‹œ
    if good_lo is not None and good_hi is not None:
        ax.axvspan(good_lo, good_hi, color="green", alpha=0.20,
                   label=f"ê¶Œì¥ êµ¬ê°„ {good_lo:.2f} â‰¤ x â‰¤ {good_hi:.2f}")

    ax.set_title(f"mold_code {TARGET_MOLD} â€” PDP: {TARGET_FEATURE} (+ ê¶Œì¥ êµ¬ê°„)", pad=16)
    ax.set_ylabel("ë¶ˆëŸ‰ í™•ë¥  (predict_proba)")
    ax.legend()
    plt.tight_layout()
    plt.show()

    # -----------------------------
    # 4. Actionable Insight ì¶œë ¥
    # -----------------------------
    print("ğŸ“Œ Actionable Insight")
    if good_lo is not None and good_hi is not None:
        print(f"- ê¶Œì¥ êµ¬ê°„: {TARGET_FEATURE} {good_lo:.2f} â‰¤ x â‰¤ {good_hi:.2f}")
        print(f"  (ì¡°ê±´: PDP ìµœì €ì¹˜ + {EPS}, í‘œë³¸ ìˆ˜ â‰¥ {MIN_BIN})")
    else:
        print(f"- ê¶Œì¥ êµ¬ê°„: ì—†ìŒ (í‘œë³¸ ë¶€ì¡± ë˜ëŠ” ê³¡ì„ ì´ í‰íƒ„)")
